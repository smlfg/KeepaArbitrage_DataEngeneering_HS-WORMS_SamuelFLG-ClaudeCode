"""
Scheduler Service for Keeper System
Runs automatic price checks every 6 hours (configurable)
With Kafka + Elasticsearch integration for Data Engineering pipeline
"""

import asyncio
import logging
import uuid
from datetime import datetime, timedelta
from typing import List, Dict, Any
from itertools import cycle

from src.services.database import (
    init_db,
    get_active_watches,
    update_watch_price,
    create_price_alert,
    mark_alert_sent,
    get_pending_alerts_with_context,
    get_active_deal_filters_with_users,
    save_collected_deals_batch,
    get_best_deals,
    backfill_price_history_from_deals,
    get_latest_deal_price,
)
from src.services.keepa_api import KeepaAPIClient, get_keepa_client
from src.services.notification import notification_service
from src.services.kafka_producer import price_producer, deal_producer
from src.services.kafka_consumer import PriceUpdateConsumer, DealUpdateConsumer
from src.services.elasticsearch_service import es_service
from src.services.database import async_session_maker
from src.agents.alert_dispatcher import alert_dispatcher
from src.agents.deal_finder import deal_finder
from src.config import get_settings
from src.services.layout_detection import (
    detect_layout,
    classify_mismatch,
    EXPECTED_LAYOUT,
    DOMAINS as LAYOUT_DOMAINS,
    FALLBACK_KEYBOARD_CATEGORIES,
)
from src.services.keepa_client import KeepaClient

try:
    from src.utils.pipeline_logger import log_arbitrage

    _PIPELINE_LOG = True
except ImportError:
    _PIPELINE_LOG = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PriceMonitorScheduler:
    """
    Scheduler that runs automatic price checks.
    Default: Every 6 hours (21600 seconds)
    """

    # Title keywords for post-filtering deals to actual keyboards.
    # The Keepa deals API category filter (340843031) is too broad and
    # returns general "Computer & Accessories" deals. This list catches
    # keyboards in DE/EN/FR/IT/ES.
    KEYBOARD_TITLE_KEYWORDS = [
        "tastatur",
        "keyboard",
        "clavier",
        "tastiera",
        "teclado",
        "qwertz",
        "qwerty",
        "mechanisch",
        "mechanical",
        "mecanique",
        "meccanica",
        "mecanico",
        "keycap",
        "key cap",
        "cherry mx",
        "gateron",
        "kailh",
        "hot-swap",
        "hotswap",
    ]

    KEYBOARD_BRAND_WHITELIST = [
        # Premium / Enthusiast
        "logitech",
        "cherry",
        "corsair",
        "razer",
        "steelseries",
        "hyperx",
        "keychron",
        "ducky",
        "leopold",
        "varmilo",
        "das keyboard",
        "filco",
        "hhkb",
        "topre",
        "realforce",
        # Gaming
        "roccat",
        "asus",
        "msi",
        "trust gaming",
        # Mainstream
        "microsoft",
        "hp",
        "dell",
        "lenovo",
        "apple",
        "hama",
        "perixx",
        "jelly comb",
        # Mechanisch-Spezialist
        "glorious",
        "wooting",
        "nuphy",
        "akko",
        "epomaker",
        "royal kludge",
        "redragon",
        "havit",
        "iclever",
    ]

    # Search terms per market for ASIN discovery
    DISCOVERY_SEARCH_TERMS = {
        "DE": ["tastatur", "mechanische tastatur", "gaming keyboard", "qwertz tastatur", "kabellose tastatur"],
        "UK": ["keyboard", "mechanical keyboard", "gaming keyboard", "wireless keyboard"],
        "FR": ["clavier", "clavier mecanique", "clavier gaming", "clavier sans fil"],
        "IT": ["tastiera", "tastiera meccanica", "tastiera gaming", "tastiera wireless"],
        "ES": ["teclado", "teclado mecanico", "teclado gaming", "teclado inalambrico"],
    }

    def __init__(
        self,
        check_interval: int = 21600,  # 6 hours in seconds
        batch_size: int = 50,
    ):
        self.settings = get_settings()
        self.check_interval = check_interval
        self.batch_size = batch_size
        self.keepa_client = KeepaAPIClient()
        self.running = False

    def _is_keyboard_deal(self, deal: Dict[str, Any]) -> bool:
        """Post-filter: check if deal title contains keyboard-related keywords."""
        title = str(deal.get("title", "")).lower()
        return any(kw in title for kw in self.KEYBOARD_TITLE_KEYWORDS)

    def _has_whitelisted_brand(self, deal: Dict[str, Any]) -> bool:
        """Post-filter: check if deal title contains a known keyboard brand."""
        title = str(deal.get("title", "")).lower()
        return any(brand in title for brand in self.KEYBOARD_BRAND_WHITELIST)

    async def check_single_price(self, watch) -> Dict[str, Any]:
        """Check price for a single watched product"""
        try:
            result = await self.keepa_client.query_product(watch.asin)

            if result:
                current_price = result.get("current_price", 0)

                # Fallback: Product API liefert keine Preise mit diesem Key
                if current_price == 0:
                    deal_price = await get_latest_deal_price(watch.asin)
                    if deal_price and deal_price > 0:
                        current_price = deal_price
                        logger.info(f"üìä DB-Fallback: {watch.asin} ‚Üí {current_price}‚Ç¨")

                buy_box_raw = result.get("buy_box_price", None)
                buy_box = str(buy_box_raw) if buy_box_raw else None

                previous_price = watch.current_price
                price_change_percent = 0
                if previous_price and previous_price > 0:
                    price_change_percent = round(
                        ((previous_price - current_price) / previous_price) * 100, 2
                    )

                return {
                    "watch_id": str(watch.id),
                    "asin": watch.asin,
                    "success": True,
                    "current_price": current_price,
                    "buy_box_seller": buy_box,
                    "previous_price": previous_price,
                    "price_change_percent": price_change_percent,
                    "price_changed": (
                        watch.current_price is None
                        or abs(current_price - watch.current_price) > 0.01
                    ),
                    "alert_triggered": current_price > 0
                    and current_price <= watch.target_price,
                }

            return {
                "watch_id": str(watch.id),
                "asin": watch.asin,
                "success": False,
                "error": "No product data returned",
            }

        except Exception as e:
            logger.error(f"Error checking price for {watch.asin}: {e}")
            return {
                "watch_id": str(watch.id),
                "asin": watch.asin,
                "success": False,
                "error": str(e),
            }

    async def run_scheduler(self):
        """Run the continuous scheduler loop"""
        logger.info("üìÖ Price Monitor Scheduler started")
        logger.info(
            f"Check interval: {self.check_interval} seconds ({self.check_interval // 3600} hours)"
        )

        await init_db()

        # Backfill price history from existing collected deals
        try:
            backfilled = await backfill_price_history_from_deals()
            if backfilled > 0:
                logger.info(f"‚úÖ Backfilled {backfilled} price history records")
        except Exception as e:
            logger.warning(f"Backfill skipped: {e}")

        # Phase 1: Start Kafka producers (must be ready before consumers)
        kafka_ready = False
        try:
            await price_producer.start()
            await deal_producer.start()
            kafka_ready = True
            logger.info("‚úÖ Kafka producers started")
        except Exception as e:
            logger.warning(f"Could not start Kafka producers: {e}")

        # Phase 2: Start Elasticsearch (independent of Kafka)
        try:
            await es_service.connect()
            logger.info("‚úÖ Elasticsearch connected")
        except Exception as e:
            logger.warning(f"Could not connect to Elasticsearch: {e}")

        # Phase 3: Start Kafka consumers only after producers are confirmed
        if kafka_ready:
            try:
                self.price_consumer = PriceUpdateConsumer(async_session_maker)
                self.deal_consumer = DealUpdateConsumer(async_session_maker)
                await self.price_consumer.start()
                await self.deal_consumer.start()
                self._consumer_tasks = [
                    asyncio.create_task(
                        self.price_consumer.consume(), name="price-consumer"
                    ),
                    asyncio.create_task(
                        self.deal_consumer.consume(), name="deal-consumer"
                    ),
                ]
                logger.info("‚úÖ Kafka consumers started (2 consumer groups)")
            except Exception as e:
                logger.warning(f"Could not start Kafka consumers: {e}")

        self.running = True
        cycle_count = 0
        elapsed_seconds = 0

        # Phase 4: Launch background deal collector after all services ready
        asyncio.create_task(self.collect_deals_to_elasticsearch())

        # Phase 5: Launch ASIN discovery if enabled
        if self.settings.discovery_enabled:
            asyncio.create_task(self.run_asin_discovery())
            logger.info("ASIN discovery pipeline launched")

        while self.running:
            # Lazy reconnect: ensure Kafka/ES are available before each cycle
            await self._ensure_connections()

            try:
                await self.run_price_check()
            except Exception as e:
                logger.error(f"Error in price check loop: {e}")

            # Run daily deal reports once per day (time-based, not cycle-based)
            cycle_count += 1
            elapsed_seconds += self.check_interval
            if elapsed_seconds >= 86400:
                elapsed_seconds = 0
                try:
                    await self.run_daily_deal_reports()
                except Exception as e:
                    logger.error(f"Error in deal report generation: {e}")

            # Wait for next check
            logger.info(f"üí§ Sleeping for {self.check_interval} seconds...")
            await asyncio.sleep(self.check_interval)

    async def _ensure_connections(self):
        """Lazy reconnect for Kafka producers and Elasticsearch.
        Called before each price-check cycle so that a failed init
        at startup doesn't silently disable the pipeline forever."""
        # Kafka producers
        if not price_producer.producer:
            try:
                await price_producer.start()
                logger.info("üîÑ Kafka price producer reconnected")
            except Exception as e:
                logger.warning(f"Kafka price producer still unavailable: {e}")

        if not deal_producer.producer:
            try:
                await deal_producer.start()
                logger.info("üîÑ Kafka deal producer reconnected")
            except Exception as e:
                logger.warning(f"Kafka deal producer still unavailable: {e}")

        # Elasticsearch
        if not es_service.client:
            try:
                await es_service.connect()
                logger.info("üîÑ Elasticsearch reconnected")
            except Exception as e:
                logger.warning(f"Elasticsearch still unavailable: {e}")

    async def run_price_check(self) -> Dict[str, Any]:
        """
        Run a single price check for all active watches.
        Returns summary of the check run.
        Sends data to Kafka for pipeline processing.
        """
        logger.info("üîç Starting scheduled price check...")

        watches = await get_active_watches()
        logger.info(f"Found {len(watches)} active watches to check")

        results = {
            "total": len(watches),
            "successful": 0,
            "failed": 0,
            "price_changes": 0,
            "alerts_triggered": 0,
            "kafka_sent": 0,
            "es_indexed": 0,
            "watches": [],
        }

        # Parallel price checks with concurrency limit
        semaphore = asyncio.Semaphore(5)

        async def _check_with_semaphore(watch):
            async with semaphore:
                return await self.check_single_price(watch)

        check_results = await asyncio.gather(
            *[_check_with_semaphore(w) for w in watches],
            return_exceptions=True,
        )

        for i, result in enumerate(check_results):
            if isinstance(result, Exception):
                logger.error(f"Error checking price for watch: {result}")
                results["failed"] += 1
                results["watches"].append(
                    {
                        "watch_id": str(watches[i].id),
                        "asin": watches[i].asin,
                        "success": False,
                        "error": str(result),
                    }
                )
                continue

            watch = watches[i]
            if result["success"]:
                results["successful"] += 1

                # Update price in database
                await update_watch_price(
                    str(watch.id), result["current_price"], result.get("buy_box_seller")
                )

                if result.get("price_changed"):
                    results["price_changes"] += 1

                # Skip 0‚Ç¨ prices ‚Äî indicates no data, not a real price
                if result["current_price"] <= 0:
                    results["watches"].append(result)
                    continue

                # Send to Kafka for pipeline processing
                kafka_result = await price_producer.send_price_update(
                    asin=watch.asin,
                    product_title=watch.asin,
                    current_price=result["current_price"],
                    target_price=watch.target_price,
                    previous_price=result.get("previous_price"),
                )
                if kafka_result:
                    results["kafka_sent"] += 1

                # Index in Elasticsearch for analytics
                try:
                    await es_service.index_price_update(
                        {
                            "asin": watch.asin,
                            "product_title": watch.asin,
                            "current_price": result["current_price"],
                            "target_price": watch.target_price,
                            "previous_price": result.get("previous_price"),
                            "price_change_percent": result.get(
                                "price_change_percent", 0
                            ),
                            "domain": "de",
                            "currency": "EUR",
                            "timestamp": datetime.utcnow().isoformat(),
                            "event_type": "price_update",
                        }
                    )
                    results["es_indexed"] += 1
                except Exception as e:
                    logger.debug(f"ES indexing skipped: {e}")

                # Trigger alert if price dropped below target
                if result.get("alert_triggered"):
                    results["alerts_triggered"] += 1

                    # Create alert record
                    alert = await create_price_alert(
                        str(watch.id), result["current_price"], watch.target_price
                    )

                    logger.info(
                        f"üö® ALERT: {watch.asin} dropped to {result['current_price']}‚Ç¨ "
                        f"(target: {watch.target_price}‚Ç¨)"
                    )

            else:
                results["failed"] += 1

            results["watches"].append(result)

        logger.info(
            f"‚úÖ Price check complete: {results['successful']} successful, "
            f"{results['failed']} failed, {results['price_changes']} price changes, "
            f"{results['alerts_triggered']} alerts, {results['kafka_sent']} to Kafka, "
            f"{results['es_indexed']} in ES"
        )

        # Pipeline logging: arbitrage stage
        if _PIPELINE_LOG:
            log_arbitrage(opportunities_found=results["alerts_triggered"])

        # Dispatch pending alerts with user/channel context
        try:
            pending = await get_pending_alerts_with_context()
            dispatched = 0
            for item in pending:
                alert = item["alert"]
                watch = item["watch"]
                user = item["user"]

                channels = ["email"]
                if user.telegram_chat_id:
                    channels.append("telegram")
                if user.discord_webhook:
                    channels.append("discord")

                alert_payload = {
                    "id": str(alert.id),
                    "user_id": str(user.id),
                    "asin": watch.asin,
                    "product_name": watch.asin,
                    "current_price": alert.triggered_price,
                    "target_price": alert.target_price,
                    "amazon_url": f"https://amazon.de/dp/{watch.asin}",
                    "channels": channels,
                    "email": user.email,
                    "telegram_chat_id": user.telegram_chat_id,
                    "discord_webhook": user.discord_webhook,
                }

                result = await alert_dispatcher.dispatch_alert(
                    alert_payload, channels=channels
                )
                if result.get("success"):
                    await mark_alert_sent(str(alert.id))
                    dispatched += 1

            if dispatched:
                logger.info(f"üìß Dispatched {dispatched} alerts")
        except Exception as e:
            logger.error(f"Error dispatching alerts: {e}")

        return results

    async def run_daily_deal_reports(self) -> Dict[str, Any]:
        """
        Run deal search for all active user filters and email the reports.
        Should be called once per day (e.g. every 4th scheduler cycle at 6h intervals).
        Also indexes deals to Elasticsearch for analytics.
        """
        logger.info("üìä Starting daily deal report generation...")

        try:
            filter_rows = await get_active_deal_filters_with_users()
        except Exception as e:
            logger.error(f"Error loading deal filters: {e}")
            return {"reports_sent": 0, "error": str(e)}

        if not filter_rows:
            logger.info("No active deal filters found, skipping deal reports")
            return {"reports_sent": 0}

        reports_sent = 0
        deals_indexed = 0

        for row in filter_rows:
            deal_filter = row["filter"]
            user = row["user"]

            filter_config = {
                "id": str(deal_filter.id),
                "name": deal_filter.name,
                "categories": deal_filter.categories or ["16142011"],
                "min_discount": deal_filter.min_discount,
                "max_discount": deal_filter.max_discount,
                "min_price": deal_filter.min_price,
                "max_price": deal_filter.max_price,
                "min_rating": deal_filter.min_rating,
            }

            try:
                deals = await deal_finder.search_deals(filter_config)
                filtered = deal_finder.filter_spam(deals)

                # Index deals to Elasticsearch for analytics
                for deal in filtered:
                    try:
                        domain = str(deal.get("domain", "de")).lower()
                        domain_id = int(deal.get("domain_id", 3) or 3)
                        market = str(deal.get("market", domain.upper())).upper()
                        await es_service.index_deal_update(
                            {
                                "asin": deal.get("asin", ""),
                                "product_title": deal.get("title", ""),
                                "current_price": deal.get("current_price", 0),
                                "original_price": deal.get("list_price", 0),
                                "discount_percent": deal.get("discount_percent", 0),
                                "rating": deal.get("rating", 0),
                                "review_count": deal.get("reviews", 0),
                                "sales_rank": deal.get("sales_rank"),
                                "domain": domain,
                                "domain_id": domain_id,
                                "market": market,
                                "timestamp": datetime.utcnow().isoformat(),
                                "event_type": "deal_update",
                                "source": deal.get("source", "product_api"),
                            }
                        )
                        deals_indexed += 1
                    except Exception as e:
                        logger.debug(f"ES deal indexing skipped: {e}")

                # Send to Kafka
                for deal in filtered:
                    domain = str(deal.get("domain", "de")).lower()
                    domain_id = int(deal.get("domain_id", 3) or 3)
                    market = str(deal.get("market", domain.upper())).upper()
                    await deal_producer.send_deal_update(
                        asin=deal.get("asin", ""),
                        product_title=deal.get("title", ""),
                        current_price=deal.get("current_price", 0),
                        original_price=deal.get("list_price", 0),
                        discount_percent=deal.get("discount_percent", 0),
                        rating=deal.get("rating", 0),
                        review_count=deal.get("reviews", 0),
                        sales_rank=deal.get("sales_rank"),
                        domain=domain,
                        domain_id=domain_id,
                        market=market,
                    )

                if deal_finder.should_send_report(filtered):
                    report_html = await deal_finder.generate_report(
                        filtered,
                        deal_filter.name,
                        f"Category: {filter_config['categories']}, "
                        f"Discount: {filter_config['min_discount']}-{filter_config['max_discount']}%, "
                        f"Price: {filter_config['min_price']}-{filter_config['max_price']}‚Ç¨",
                    )

                    result = await notification_service.send_email(
                        to=user.email,
                        subject=f"Daily Deal Report: {deal_filter.name}",
                        html_body=report_html,
                    )

                    if result.get("success"):
                        reports_sent += 1
                        logger.info(
                            f"üìß Sent deal report '{deal_filter.name}' to {user.email} "
                            f"({len(filtered)} deals)"
                        )
                    else:
                        logger.warning(
                            f"Failed to send deal report to {user.email}: {result.get('error')}"
                        )
            except Exception as e:
                logger.error(
                    f"Error generating deal report '{deal_filter.name}' for {user.email}: {e}"
                )

        logger.info(
            f"üìä Deal reports complete: {reports_sent} sent, {deals_indexed} indexed to ES"
        )
        return {"reports_sent": reports_sent, "deals_indexed": deals_indexed}

    def _load_seed_asins_from_file(self) -> list:
        """Load QWERTZ keyboard ASINs from seed file."""
        from pathlib import Path

        seed_path = (
            Path(__file__).resolve().parent.parent / "data" / "seed_asins_eu_qwertz.txt"
        )
        if not seed_path.exists():
            logger.warning("Seed file not found: %s", seed_path)
            return []

        asins = []
        for line in seed_path.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            asin = line.split()[0].upper()
            if len(asin) == 10:
                asins.append(asin)

        logger.info("Loaded %d seed ASINs from %s", len(asins), seed_path)
        return asins

    async def _collect_seed_asin_deals(self, asins: list) -> list:
        """Fallback: query individual seed ASINs via product API for current prices."""
        semaphore = asyncio.Semaphore(5)

        async def _query(asin: str):
            async with semaphore:
                result = await self.keepa_client.query_product(asin, domain_id=3)
                if result and result.get("current_price", 0) > 0:
                    deal = deal_finder._build_deal_from_product(
                        result, domain_id=3, market="DE"
                    )
                    return deal_finder._score_deal(deal)
                return None

        raw = await asyncio.gather(
            *[_query(a) for a in asins], return_exceptions=True
        )
        all_deals = []
        for item in raw:
            if isinstance(item, Exception):
                logger.debug("Seed ASIN query failed: %s", item)
            elif item is not None:
                all_deals.append(item)
        return all_deals

    async def collect_deals_to_elasticsearch(self):
        """
        Background task: collect QWERTZ keyboard deals and index to:
        1. Elasticsearch (for search & analytics)
        2. Kafka (for event streaming)
        3. PostgreSQL (for historical analysis)

        Strategy:
        - Primary: Keepa deals API with keyboard category filter (340843031)
        - Fallback: Query seed ASINs directly via product API
        """
        logger.info("üîÑ QWERTZ Keyboard Deal collector started")
        logger.info(
            "Deal collector config: interval=%ss, batch_size=%s",
            self.settings.deal_scan_interval_seconds,
            self.settings.deal_scan_batch_size,
        )

        # Amazon.de browse node for Tastaturen (Keyboards)
        KEYBOARD_CATEGORY_ID = 340843031

        # Keyboard-specific deal filters
        batch_size = max(1, int(self.settings.deal_scan_batch_size or 50))
        deal_configs = [
            {
                "min_discount": 10,
                "max_discount": 90,
                "min_price": 15,
                "max_price": 300,
                "min_rating": 3.5,
                "categories": [KEYBOARD_CATEGORY_ID],
                "max_asins": batch_size,
            },
        ]

        # Load seed ASINs for fallback
        seed_asins = self._load_seed_asins_from_file()

        collection_cycle = 0
        seed_cursor = 0

        while self.running:
            try:
                config_idx = collection_cycle % len(deal_configs)
                config = dict(deal_configs[config_idx])
                config["start_offset"] = seed_cursor
                all_deals = await deal_finder.search_deals(config)

                # Post-filter: only keep deals with keyboard-related titles
                if all_deals:
                    before_filter = len(all_deals)
                    all_deals = [d for d in all_deals if self._is_keyboard_deal(d)]
                    filtered_out = before_filter - len(all_deals)
                    if filtered_out > 0:
                        logger.info(
                            "Keyboard title filter: %d/%d kept, %d non-keyboard deals removed",
                            len(all_deals),
                            before_filter,
                            filtered_out,
                        )

                # Brand whitelist filter: only keep known keyboard brands
                if all_deals:
                    before_brand = len(all_deals)
                    all_deals = [d for d in all_deals if self._has_whitelisted_brand(d)]
                    brand_filtered = before_brand - len(all_deals)
                    if brand_filtered > 0:
                        logger.info(
                            "Brand whitelist filter: %d/%d kept, %d unknown brands removed",
                            len(all_deals),
                            before_brand,
                            brand_filtered,
                        )

                # Fallback: if deals API returned nothing, query seed ASINs directly
                if not all_deals and seed_asins:
                    logger.info(
                        "Deals API returned no keyboard deals, falling back to seed ASINs"
                    )
                    batch_start = seed_cursor % len(seed_asins)
                    batch_end = min(batch_start + batch_size, len(seed_asins))
                    batch = seed_asins[batch_start:batch_end]
                    if not batch:
                        batch = seed_asins[:batch_size]
                    all_deals = await self._collect_seed_asin_deals(batch)

                if not all_deals:
                    logger.info("No keyboard deals collected this cycle")
                else:
                    # 1. Index to Elasticsearch
                    es_indexed = 0
                    for deal in all_deals:
                        try:
                            domain = str(deal.get("domain", "de")).lower()
                            domain_id = int(deal.get("domain_id", 3) or 3)
                            market = str(deal.get("market", domain.upper())).upper()
                            await es_service.index_deal_update(
                                {
                                    "asin": deal.get("asin", ""),
                                    "product_title": deal.get("title", ""),
                                    "current_price": deal.get("current_price", 0),
                                    "original_price": deal.get("list_price", 0),
                                    "discount_percent": deal.get("discount_percent", 0),
                                    "rating": deal.get("rating", 0),
                                    "review_count": deal.get("reviews", 0),
                                    "sales_rank": deal.get("sales_rank"),
                                    "domain": domain,
                                    "domain_id": domain_id,
                                    "market": market,
                                    "timestamp": datetime.utcnow().isoformat(),
                                    "event_type": "deal_collector",
                                    "source": deal.get("source", "product_api"),
                                }
                            )
                            es_indexed += 1
                        except Exception:
                            pass

                    # 2. Send to Kafka
                    kafka_sent = 0
                    for deal in all_deals:
                        try:
                            domain = str(deal.get("domain", "de")).lower()
                            domain_id = int(deal.get("domain_id", 3) or 3)
                            market = str(deal.get("market", domain.upper())).upper()
                            await deal_producer.send_deal_update(
                                asin=deal.get("asin", ""),
                                product_title=deal.get("title", ""),
                                current_price=deal.get("current_price", 0),
                                original_price=deal.get("list_price", 0),
                                discount_percent=deal.get("discount_percent", 0),
                                rating=deal.get("rating", 0),
                                review_count=deal.get("reviews", 0),
                                sales_rank=deal.get("sales_rank"),
                                domain=domain,
                                domain_id=domain_id,
                                market=market,
                            )
                            kafka_sent += 1
                        except Exception:
                            pass

                    # 3. Save to PostgreSQL
                    db_ready_deals = []
                    for deal in all_deals:
                        domain = str(deal.get("domain", "de")).lower()
                        db_ready_deals.append(
                            {
                                "asin": deal.get("asin", ""),
                                "title": deal.get("title", ""),
                                "current_price": deal.get("current_price", 0),
                                "original_price": deal.get("list_price", 0),
                                "discount_percent": deal.get("discount_percent", 0),
                                "rating": deal.get("rating", 0),
                                "review_count": deal.get("reviews", 0),
                                "sales_rank": deal.get("sales_rank"),
                                "domain": domain,
                                "category": deal.get("category"),
                                "url": deal.get("url"),
                                "prime_eligible": deal.get("prime_eligible", False),
                                "deal_score": deal.get("deal_score"),
                            }
                        )
                    db_saved = await save_collected_deals_batch(db_ready_deals)

                    logger.info(
                        f"üì¶ Keyboard deal collection #{collection_cycle + 1}: "
                        f"ES={es_indexed}, Kafka={kafka_sent}, DB={db_saved} deals"
                    )

                seed_cursor += int(config.get("max_asins", batch_size) or batch_size)
                logger.info(
                    "Deal collector cursor advanced to %s (step=%s)",
                    seed_cursor,
                    int(config.get("max_asins", batch_size) or batch_size),
                )

                collection_cycle += 1

            except Exception as e:
                logger.warning(f"Deal collector error: {e}")

            interval = max(30, int(self.settings.deal_scan_interval_seconds or 300))
            await asyncio.sleep(interval)

    # ===== ASIN Discovery Pipeline =====

    def _load_existing_asins_from_csv(self) -> set:
        """Load already-discovered ASINs from the targets CSV to avoid duplicates."""
        from pathlib import Path
        import csv as csv_mod

        csv_path = Path(__file__).resolve().parent.parent / "data" / "discovered_asins.csv"
        if not csv_path.exists():
            return set()

        existing = set()
        with open(csv_path, encoding="utf-8") as f:
            reader = csv_mod.DictReader(f)
            for row in reader:
                asin = row.get("asin", "").strip().upper()
                if asin:
                    existing.add(asin)
        return existing

    def _append_discovered_asins(self, new_asins: list, existing: set) -> int:
        """Append newly discovered ASINs to discovered_asins.csv. Returns count appended."""
        from pathlib import Path
        import csv as csv_mod

        csv_path = Path(__file__).resolve().parent.parent / "data" / "discovered_asins.csv"
        write_header = not csv_path.exists()

        fieldnames = [
            "asin", "market", "domain_id", "title", "detected_layout",
            "expected_layout", "is_mismatch", "confidence", "detection_layer",
            "discovery_strategy", "discovered_at",
        ]

        appended = 0
        with open(csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv_mod.DictWriter(f, fieldnames=fieldnames)
            if write_header:
                writer.writeheader()
            for entry in new_asins:
                asin = entry.get("asin", "").upper()
                if asin and asin not in existing:
                    writer.writerow(entry)
                    existing.add(asin)
                    appended += 1

        return appended

    def _get_discovery_search_terms(self) -> list:
        """Flatten all discovery search terms into (market, domain_id, term) tuples."""
        terms = []
        for market, term_list in self.DISCOVERY_SEARCH_TERMS.items():
            domain_id = LAYOUT_DOMAINS.get(market, 3)
            for term in term_list:
                terms.append((market, domain_id, term))
        return terms

    async def run_asin_discovery(self):
        """
        Continuous ASIN discovery pipeline.
        Rotates through search/bestseller/product_finder strategies
        across 5 EU markets. Appends new ASINs with layout detection to CSV.
        Uses a separate light KeepaClient (httpx, ~5-10 tokens/call).
        """
        logger.info("ASIN discovery pipeline started")
        logger.info(
            "Discovery config: interval=%ss, token_reserve=%s",
            self.settings.discovery_interval_seconds,
            self.settings.discovery_token_reserve,
        )

        light_client = KeepaClient()
        search_terms = self._get_discovery_search_terms()
        strategies = cycle(["search", "bestsellers", "product_finder"])
        term_idx = 0
        discovery_cycle = 0

        while self.running:
            try:
                existing = self._load_existing_asins_from_csv()
                strategy = next(strategies)
                market, domain_id, term = search_terms[term_idx % len(search_terms)]
                term_idx += 1

                # Token budget check before each call
                if light_client.rate_limit_remaining < self.settings.discovery_token_reserve:
                    reset_ms = light_client.rate_limit_reset or 15000
                    wait_s = min(max(reset_ms / 1000, 10), 120)
                    logger.info(
                        "Discovery: tokens low (%s), pausing %ss",
                        light_client.rate_limit_remaining,
                        int(wait_s),
                    )
                    await asyncio.sleep(wait_s)
                    continue

                raw_asins = []

                if strategy == "search":
                    try:
                        response = await light_client.search_products(
                            search_term=term,
                            domain_id=domain_id,
                        )
                        products = response.get("products", [])
                        if isinstance(products, list):
                            raw_asins = products
                    except Exception as e:
                        logger.debug("Discovery search error: %s", e)

                elif strategy == "bestsellers":
                    try:
                        cat_id = FALLBACK_KEYBOARD_CATEGORIES.get(market, 340843031)
                        response = await light_client.get_bestsellers(
                            domain_id=domain_id,
                            category=cat_id,
                        )
                        asin_list = response.get("bestSellersList", {})
                        if isinstance(asin_list, dict):
                            for asins in asin_list.values():
                                if isinstance(asins, list):
                                    raw_asins.extend(
                                        [{"asin": a} for a in asins if isinstance(a, str)]
                                    )
                        elif isinstance(asin_list, list):
                            raw_asins = [{"asin": a} for a in asin_list if isinstance(a, str)]
                    except Exception as e:
                        logger.debug("Discovery bestsellers error: %s", e)

                elif strategy == "product_finder":
                    try:
                        cat_id = FALLBACK_KEYBOARD_CATEGORIES.get(market, 340843031)
                        response = await light_client.product_finder(
                            domain_id=domain_id,
                            product_parms={
                                "rootCategory": cat_id,
                                "minPrice": 800,
                                "maxPrice": 80000,
                            },
                        )
                        asin_list = response.get("asinList", [])
                        if isinstance(asin_list, list):
                            raw_asins = [{"asin": a} for a in asin_list if isinstance(a, str)]
                    except Exception as e:
                        logger.debug("Discovery product_finder error: %s", e)

                # Process discovered ASINs through layout detection
                new_entries = []
                for item in raw_asins:
                    asin = ""
                    if isinstance(item, dict):
                        asin = item.get("asin", "").upper()
                    elif isinstance(item, str):
                        asin = item.upper()

                    if not asin or len(asin) != 10 or asin in existing:
                        continue

                    # Build entry for layout detection
                    entry = {
                        "title": item.get("title", "") if isinstance(item, dict) else "",
                        "brand": item.get("brand", "") if isinstance(item, dict) else "",
                        "ean": "",
                        "present_markets": {market},
                    }

                    detection = detect_layout(entry)
                    expected = EXPECTED_LAYOUT.get(market, "")
                    is_mismatch, _ = classify_mismatch(
                        detection["detected_layout"], market
                    )

                    new_entries.append({
                        "asin": asin,
                        "market": market,
                        "domain_id": domain_id,
                        "title": entry["title"][:200],
                        "detected_layout": detection["detected_layout"],
                        "expected_layout": expected,
                        "is_mismatch": is_mismatch,
                        "confidence": detection["confidence"],
                        "detection_layer": detection["detection_layer"],
                        "discovery_strategy": strategy,
                        "discovered_at": datetime.utcnow().isoformat(),
                    })

                if new_entries:
                    appended = self._append_discovered_asins(new_entries, existing)
                    if appended > 0:
                        logger.info(
                            "Discovery #%d [%s/%s/%s]: +%d new ASINs (total: %d)",
                            discovery_cycle + 1,
                            strategy,
                            market,
                            term[:20],
                            appended,
                            len(existing),
                        )
                else:
                    logger.debug(
                        "Discovery #%d [%s/%s/%s]: no new ASINs",
                        discovery_cycle + 1,
                        strategy,
                        market,
                        term[:20],
                    )

                discovery_cycle += 1

            except Exception as e:
                logger.warning("Discovery pipeline error: %s", e)

            await asyncio.sleep(
                max(30, self.settings.discovery_interval_seconds)
            )

    async def async_stop(self):
        """Gracefully stop all services in reverse startup order"""
        self.running = False
        logger.info("üõë Stopping scheduler...")

        # 1. Stop consumers first (they depend on producers/db)
        if hasattr(self, "price_consumer"):
            await self.price_consumer.stop()
        if hasattr(self, "deal_consumer"):
            await self.deal_consumer.stop()

        # Wait for consumer tasks to finish
        if hasattr(self, "_consumer_tasks"):
            for task in self._consumer_tasks:
                task.cancel()
            await asyncio.gather(*self._consumer_tasks, return_exceptions=True)

        # 2. Stop producers
        await price_producer.stop()
        await deal_producer.stop()

        # 3. Close Elasticsearch
        await es_service.close()

        logger.info("üõë All services stopped")

    def stop(self):
        """Stop the scheduler (sync wrapper)"""
        self.running = False
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(self.async_stop())
            else:
                loop.run_until_complete(self.async_stop())
        except Exception as e:
            logger.debug(f"Cleanup: {e}")


async def run_immediate_check():
    """Run a single price check (for manual trigger)"""
    await init_db()
    scheduler = PriceMonitorScheduler(check_interval=21600)
    return await scheduler.run_price_check()


async def check_single_asin(asin: str) -> Dict[str, Any]:
    """Check price for a single ASIN (for manual query)"""
    await init_db()
    client = KeepaAPIClient()
    return await client.query_product(asin)


if __name__ == "__main__":
    print("Starting Keeper Price Monitor Scheduler...")
    scheduler = PriceMonitorScheduler(check_interval=21600)

    try:
        asyncio.run(scheduler.run_scheduler())
    except KeyboardInterrupt:
        scheduler.stop()
        print("Scheduler stopped.")
